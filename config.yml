TEST                   : false    # Start in Train mode or Test mode
DISPLAY                : true     # Whether to use display or not

IMAGE_HEIGHT           : 105      # Input frame Height
IMAGE_WIDTH            : 80       # Input frame Width
GRAYSCALE_IMG          : false
NORMALIZE_IMG          : true
IMAGE_CROPING          : false    # In the original paper a square croped region is used however it is optional
                                  # NOTE: Cropping assumes height is always greater than width

REPEAT_ACTION          : 1        # Number of times the same action is repeated.
                                  # Explicit frame skipping is not required in many gym games
                                  # since it is already implemented.

AGENT_TYPE             : "DQN"    # DQN, DRQN TODO
DOUBLE_Q               : false    # Enable Double Q-Learning

ENVIRONMENT:
    NAME                   : "Breakout-v0"

AGENT:
    GAMMA                  : 0.99     # Discount factor
    STATE_LENGTH           : 4        # Number of most recent frames to produce the input to the network
    INITIAL_EPSILON        : 1.0      # Initial value of epsilon in epsilon-greedy
    FINAL_EPSILON          : 0.1      # Final value of epsilon in epsilon-greedy
    INITIAL_REPLAY_SIZE    : 10000    # Number of steps to populate the replay memory before training starts
    EXPLORATION_STEPS      : 50000   # Number of exploratoion steps
    MEMORY_SIZE            : 100000   # Number of replay memory the agent uses for training

    BATCH_SIZE             : 64       # Mini batch size
    TARGET_UPDATE_INTERVAL : 10000    # The frequency with which the target network is updated
    TRAIN_INTERVAL         : 4        # The agent selects 4 actions between successive updates

    LEARNING_RATE          : 0.00025  # Learning rate used by RMSProp
    MOMENTUM               : 0.95     # Momentum used by RMSProp
    MIN_GRAD               : 0.01     # Constant added to the squared gradient in the denominator of the RMSProp update

    SAVE_INTERVAL          : 10000    # The frequency with which the network is saved
    SAVE_MEMORY            : false    # Take Snapshot of Experience Replay Memory
    LOAD_NETWORK           : true     # Whether to load model from SAVE_NETWORK_PATH or train a new one
    SAVE_NETWORK_PATH      : "models/"
    SAVE_SUMMARY_PATH      : "logs/"
